{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGD - Loss Given Default\n",
    "This notebook will train a model to predict Loss Given Default (LGD). Let's start with feature selection\n",
    "\n",
    "**LGD** indicates the proportion of the total exposure that remains unrecovered after a borrower has defaulted.\n",
    "\n",
    "Hence, it is a better practice to build the model with data that borrowers have a `charged off` status (status of loss)\n",
    "\n",
    "From the data provided, we know that:\n",
    "\n",
    "* `funded_amnt`: reflects the total amount that was lost at the moment the borrower defaulted;\n",
    "\n",
    "* `recoveries`: amount that has been recovered.\n",
    "\n",
    "Hence, LGD can be defined as:\n",
    "\n",
    "$LGD = \\frac{funded\\_amnt - recoveries}{funded\\_amnt} = 1 - recovery\\_rate$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Data Science\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Data Visualization\n",
    "import seaborn as sns\n",
    "import plotly.io as pio # plotly is only used if you have a powerfull machine\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "# ignore all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../data/train/train.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only get records that have `charged off` status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67068, 24)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEFAULT_CATEGORIES = [\n",
    "    \"Charged Off\",\n",
    "    \"Does not meet the credit policy. Status:Charged Off\"\n",
    "]\n",
    "\n",
    "train_df = train_df[train_df[\"loan_status\"].isin(DEFAULT_CATEGORIES)]\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    67068.000000\n",
       "mean         0.922581\n",
       "std          0.097508\n",
       "min         -0.294209\n",
       "25%          0.896070\n",
       "50%          0.937282\n",
       "75%          1.000000\n",
       "max          1.000000\n",
       "Name: LGD, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TARGET_VARIABLE = \"LGD\"\n",
    "\n",
    "train_df[TARGET_VARIABLE] = 1 - (train_df[\"recoveries\"] / train_df[\"funded_amnt\"])\n",
    "train_df[TARGET_VARIABLE].describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will remove all the LGD that smaller than 0 because there is no loss on those borrower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67043, 25)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = train_df[train_df[TARGET_VARIABLE]>0]\n",
    "train_df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the distribution of the LGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = ff.create_distplot([train_df[TARGET_VARIABLE]], [TARGET_VARIABLE], bin_size=0.1)\n",
    "# fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can easily notice that our target variable is unevenly distributed, with a lot of values concentrated around 1 because default events in this data more likely happen."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Analysis\n",
    "We are going to use features from PD model for alignment. However, we should analyze each features before coming up an appropriate regression model. Let's start by transforming features\n",
    "\n",
    "### Feature Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'purpos'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Minh\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3620\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3621\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3622\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Minh\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Minh\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'purpos'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_36768\\3819612730.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[0mreverse_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mpurpose\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mgroup\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpurposes\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mPURPOSES_MAP\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mpurpose\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpurposes\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m \u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"purpose\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"purpos\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreverse_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[0mencoded_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"purpos\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"purpos\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Minh\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3503\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3504\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3505\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3506\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3507\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Minh\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3621\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3622\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3623\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3624\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3625\u001b[0m                 \u001b[1;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'purpos'"
     ]
    }
   ],
   "source": [
    "SELECTED_FEATURES = [\n",
    "    \"id\", \"purpose\", \"initial_list_status\", \"emp_length\",\n",
    "    \"pub_rec\", \"grade\", \"addr_state\", \"term\",\n",
    "    \"int_rate\", \"LGD\"\n",
    "]\n",
    "train_df = train_df[SELECTED_FEATURES]\n",
    "\n",
    "encoded_df = pd.get_dummies(train_df['grade'], prefix='grade')\n",
    "train_df = train_df.drop('grade', axis=1)\n",
    "train_df = pd.concat([train_df, encoded_df], axis=1)\n",
    "\n",
    "ADDR_STATE_MAP = {\n",
    "    \"ID\": [\"ID\"],\n",
    "    \"NE_ME\": [\"NE\",\"ME\"],\n",
    "    \"ND\": [\"ND\"],\n",
    "    \"MS_AR_OK\": [\"MS\", \"OK\", \"AR\"],\n",
    "    \"SD_LA_AL_AK_IN\": [\"SD\", \"LA\", \"AL\", \"AK\", \"IN\"],\n",
    "    \"WV_MO_MD_OH\": [\"WV\", \"MO\", \"MD\", \"OH\"],\n",
    "    \"NY_TN_KY_NJ_SC_NC_CT\": [\"NY\", \"TN\", \"KY\", \"NJ\", \"SC\", \"NC\", \"CT\"],\n",
    "    \"IL\": [\"IL\"],\n",
    "    \"PA_NM\": [\"PA\", \"NM\"],\n",
    "    \"TX_VT\": [\"TX\", \"VT\"],\n",
    "    \"FL_NV_GA\": [\"FL\", \"NV\", \"GA\"],\n",
    "    \"VA_MN\": [\"MN\", \"VA\"],\n",
    "    \"KS_DE_MA_MI_MT\": [\"KS\", \"DE\", \"MA\", \"MI\", \"MT\"],\n",
    "    \"AZ_HI_WY_WI\": [\"AZ\", \"HI\", \"WY\", \"WI\"],\n",
    "    \"RI_CA_NH\": [\"RI\", \"CA\", \"NH\"],\n",
    "    \"WA_CO_OR_UT\": [\"WA\", \"CO\", \"OR\", \"UT\"],\n",
    "    \"DC\": [\"DC\"]\n",
    "}\n",
    "\n",
    "reverse_map = {state: group for group, states in ADDR_STATE_MAP.items() for state in states}\n",
    "\n",
    "train_df[\"addr_state\"] = train_df[\"addr_state\"].map(reverse_map)\n",
    "\n",
    "encoded_df = pd.get_dummies(train_df[\"addr_state\"], prefix=\"addr_state\")\n",
    "train_df = train_df.drop(\"addr_state\", axis=1)\n",
    "train_df = pd.concat([train_df, encoded_df], axis=1)\n",
    "\n",
    "PURPOSES_MAP = {\n",
    "    \"cred_card\": [\"credit_card\"],\n",
    "    \"vacation\": [\"vacation\"],\n",
    "    \"car\": [\"car\"],\n",
    "    \"home_improv_debt_consol\": [\"home_improvement\", \"debt_consolidation\"],\n",
    "    \"moving\": [\"moving\"],\n",
    "    \"renewable_energy\": [\"renewable_energy\"],\n",
    "    \"major_purchase_medical\": [\"major_purchase\", \"medical\"],\n",
    "    \"other\": [\"other\"],\n",
    "    \"wedding\": [\"wedding\"],\n",
    "    \"small_business\": [\"small_business\"],\n",
    "    \"house\": [\"house\"]\n",
    "}\n",
    "\n",
    "reverse_map = {purpose: group for group, purposes in PURPOSES_MAP.items() for purpose in purposes}\n",
    "train_df[\"purpose\"] = train_df[\"purpose\"].map(reverse_map)\n",
    "\n",
    "encoded_df = pd.get_dummies(train_df[\"purpose\"], prefix=\"purpose\")\n",
    "train_df = train_df.drop(\"purpose\", axis=1)\n",
    "train_df = pd.concat([train_df, encoded_df], axis=1)\n",
    "\n",
    "encoded_df = pd.get_dummies(train_df[\"term\"], prefix=\"term\")\n",
    "train_df = train_df.drop(\"term\", axis=1)\n",
    "train_df = pd.concat([train_df, encoded_df], axis=1)\n",
    "\n",
    "encoded_df = pd.get_dummies(train_df[\"initial_list_status\"], prefix=\"initial_list_status\")\n",
    "train_df = train_df.drop(\"initial_list_status\", axis=1)\n",
    "train_df = pd.concat([train_df, encoded_df], axis=1)\n",
    "\n",
    "\n",
    "EMP_LENGTH_MAP = {\n",
    "    \"0\": 0,\n",
    "    \"< 1 year\": 1,\n",
    "    \"1 year\": 1,\n",
    "    \"2 years\": 2,\n",
    "    \"3 years\": 3,\n",
    "    '4 years': 4,\n",
    "    '5 years': 5,\n",
    "    '6 years': 6,\n",
    "    '7 years': 7,\n",
    "    '8 years': 8,\n",
    "    '9 years': 9,\n",
    "    '10+ years': 10\n",
    "}\n",
    "train_df[\"emp_length\"] = train_df[\"emp_length\"].map(EMP_LENGTH_MAP)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Features\n",
    "Since we use one-hot encoding for the categorical features, we should check the distribution of each dummy feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DUMMIES_PREFIX = [\"grade\", \"addr_state\", \"term\", \"purpose\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DUMMIES_PREFIX' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_34888\\400876209.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mprefix\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mDUMMIES_PREFIX\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mdummy_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdummy_cols\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DUMMIES_PREFIX' is not defined"
     ]
    }
   ],
   "source": [
    "for prefix in DUMMIES_PREFIX:\n",
    "    dummy_cols = [col for col in train_df.columns if col.startswith(prefix)]\n",
    "    \n",
    "    for col in dummy_cols:\n",
    "\n",
    "        group0 = train_df[train_df[col] == 0][TARGET_VARIABLE]\n",
    "        group1 = train_df[train_df[col] == 1][TARGET_VARIABLE]\n",
    "\n",
    "        plt.figure(figsize=(16, 9)) \n",
    "        sns.kdeplot(group0, label=f'{col}=0')\n",
    "        sns.kdeplot(group1, label=f'{col}=1')\n",
    "        plt.legend()\n",
    "        plt.title(f'Density plot of {TARGET_VARIABLE} for {col}')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
